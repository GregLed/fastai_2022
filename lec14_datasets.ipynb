{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,numpy as np,matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "from itertools import zip_longest\n",
    "import fastcore.all as fc\n",
    "\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "from miniai.training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging,pickle,gzip,os,time,shutil,torch,matplotlib as mpl\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "     \n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of\n",
    "60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\n",
    "associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\n",
    "replacement for the original MNIST dataset for benchmarking machine learning algorithms.\n",
    "It shares the same image size and structure of training and testing splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_dataset_builder is used to insect the data before loading it to your filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "name = \"fashion_mnist\"\n",
    "ds_builder = load_dataset_builder(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': Image(mode=None, decode=True, id=None),\n",
      " 'label': ClassLabel(names=['T - shirt / top',\n",
      "                            'Trouser',\n",
      "                            'Pullover',\n",
      "                            'Dress',\n",
      "                            'Coat',\n",
      "                            'Sandal',\n",
      "                            'Shirt',\n",
      "                            'Sneaker',\n",
      "                            'Bag',\n",
      "                            'Ankle boot'],\n",
      "                     id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(ds_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': SplitInfo(name='test',\n",
      "                   num_bytes=5235160,\n",
      "                   num_examples=10000,\n",
      "                   shard_lengths=None,\n",
      "                   dataset_name='fashion_mnist'),\n",
      " 'train': SplitInfo(name='train',\n",
      "                    num_bytes=31304707,\n",
      "                    num_examples=60000,\n",
      "                    shard_lengths=None,\n",
      "                    dataset_name='fashion_mnist')}\n"
     ]
    }
   ],
   "source": [
    "pprint(ds_builder.info.splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DataSet Dict from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd = load_dataset(name)\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dsd[\"train\"], dsd[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       " 'label': 9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('image', 'label')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = ds_builder.info.features\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tbw1oNx4m8QWmkWx2yXD4LkZCADJJ+gFbviL4a63oc7COE3MW4hdn38duD976jNc9daDqllIsc9lKrMu4YGeMkdR7gj8KzcV7H8BtEvV16+1iWCeG1Wz8mOV02pIzupwCeuAp6Z98cZ90aIzLIlw0c0ZJ4KgjHoeOa+evjS9n/wnMcNxBPCYLKONFhA2FNzMpGenDcgd816V4K03wefC+m3NlpVhP+5QSXBiR5fMx825iMg5zwce3FdbOzTwgW90lu6uCm8eYrL02soIyCPQgggEdMGQ3cluiPNK0rJwrRQBNueuMkt+teNfGKxsdY8WWdxNqcNo66eieXMwVsb5DnH415Hp2rajpE5n02/urOUjBe3laMkehIPIrVm8eeLrhNknibVivoLtx/I1UPinxC3XXtUP1vJP8ay5JZJpGkldnduSzHJP41//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACD0lEQVR4AbWRz2sTQRTH38zszm42k6Q2TU2ixURstRdRasWC9FCsUE9SpIKnKl48+x94UOjNiyf/h4KgKN7TYgNKK+agjZS0wZDYbND90dn54VqxIWfxXebBB97nfecB/P9CAKkFAGT8ViEEh+9fLZZn7gde+E4AwkgAGYBEzl3btZz55y0tgSl/AHKYLhH85uJKdat2ebqyFmuOCun5laFIwcYXjvLRxq1nfRh3er0ESHAI1fvPYqF8oj9WxxO6hcAyWZhQV2fw6OvBbcEh2O/tlxTCjlRjgPtGAqwYcpN7GWUlPbppXepDTeB2AduQHONWpOzsi09GHxocPobUccZPh8RhcvfOy/V4IUQwigMIgFdeQHWb2BFEipzvxU6iBT9QALNPq8F3oYTnSRt8oN4iHOYcLk4UFs+GOEo0TZrlToXNqp7ZmkQw8yg3JIkrHI6C2lI1dawE9dQPP8HSDiJrRSF9IAFAZmT5+oNm+LU+nuVmispT6N6TbcYsMDONZg7nb9rl5NQU5pgCMq8YrUY6bDCa3t9hQShWt0rD3I0kNxWiE8aebiRH3E7bsEw7hTuTXqNrdSIRJfK9C8aH1bvNesioTcmB1P43JY2QcdeNRLkVR7nx8HjblYQaBGnTpCYC1AKq8ptLCMf55x6PZjAxJGrpPfWTgI58/LZW+fMJ8WXO5bond/j20Y3+sfkFaCTYdrBYeB0AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train[0][x]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 0, 0, 3, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = train[:5][x]\n",
    "yb = train[:5][y]\n",
    "\n",
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to now know which IDs map to their corresponding labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['T - shirt / top',\n",
      "                  'Trouser',\n",
      "                  'Pullover',\n",
      "                  'Dress',\n",
      "                  'Coat',\n",
      "                  'Sandal',\n",
      "                  'Shirt',\n",
      "                  'Sneaker',\n",
      "                  'Bag',\n",
      "                  'Ankle boot'],\n",
      "           id=None)\n"
     ]
    }
   ],
   "source": [
    "featy = train.features[y]\n",
    "pprint(featy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ankle boot',\n",
       " 'T - shirt / top',\n",
       " 'T - shirt / top',\n",
       " 'Dress',\n",
       " 'T - shirt / top']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featy.int2str(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we can do all of our work in the collation function\n",
    "# 1. go through each item in a batch\n",
    "# 2. grab x from each item\n",
    "# 3. convert it to a tensor\n",
    "# 4. stack them together into one tensor\n",
    "\n",
    "def collate_fn(b):\n",
    "    return {x:torch.stack([TF.to_tensor(o[x]) for o in b]),\n",
    "            y:tensor([o[y] for o in b])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 28, 28]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "\n",
    "dl = DataLoader(train, collate_fn=collate_fn, batch_size=bs)\n",
    "b = next(iter(dl))\n",
    "b[x].shape,b[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second approach is to use with_transform method that HF datasets have and it allows us to change transform a batch of data. Note that we need to return a new transformed batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(b):\n",
    "    b[x] = [TF.to_tensor(o) for o in b[x]]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 28, 28]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = train.with_transform(transforms)\n",
    "dl = DataLoader(tds, batch_size=16)\n",
    "b = next(iter(dl))\n",
    "b[x].shape,b[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n",
    "\n",
    "def inplace(f):\n",
    "    def _f(b):\n",
    "        f(b)\n",
    "        return b\n",
    "    return _f\n",
    "\n",
    "transformi = inplace(_transformi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), 9)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = train.with_transform(transformi)[0]\n",
    "r[x].shape,r[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "flatten(input, start_dim=0, end_dim=-1) -> Tensor\n",
      "\n",
      "Flattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim`\n",
      "are passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened.\n",
      "The order of elements in :attr:`input` is unchanged.\n",
      "\n",
      "Unlike NumPy's flatten, which always copies input's data, this function may return the original object, a view,\n",
      "or copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can\n",
      "be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the\n",
      "flattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned.\n",
      "\n",
      ".. note::\n",
      "    Flattening a zero-dimensional tensor will return a one-dimensional view.\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "    start_dim (int): the first dim to flatten\n",
      "    end_dim (int): the last dim to flatten\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> t = torch.tensor([[[1, 2],\n",
      "    ...                    [3, 4]],\n",
      "    ...                   [[5, 6],\n",
      "    ...                    [7, 8]]])\n",
      "    >>> torch.flatten(t)\n",
      "    tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "    >>> torch.flatten(t, start_dim=1)\n",
      "    tensor([[1, 2, 3, 4],\n",
      "            [5, 6, 7, 8]])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "torch.flatten??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
